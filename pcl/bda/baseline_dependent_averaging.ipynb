{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division, absolute_import\n",
    "import numpy as np\n",
    "from astropy import constants as const\n",
    "from astropy.coordinates import Angle\n",
    "from astropy import units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook investigates the savings that can be had when using baseline-dependent averaging (BDA). We are assuming that the proposed layout for the full HERA-350 array is fixed, as well as the correlator having the properties required for adequate imaging. Specifically, we are assuming a fringe-stopped correlator with a pre-fringe-stopped integration time of 0.1 seconds, and a post-fringe-stopped integration time of 10 seconds. We are also assuming that the magic \"allowable\" amount of decorrelation for a given baseline is 10% for a 10 degree field of view (FoV). We also assume that there are 8192 channels of output, but note that this choice does not significantly affect the possible savings versus 4096 (so in effect, the only effective change is a doubling of the base data rate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define correlator properties\n",
    "max_decorr = 0.1\n",
    "frequency = (250 * 1e6 * units.Hz)\n",
    "wavelength = const.c / frequency.to(1/units.s)\n",
    "earth_rot_speed = (Angle(360, units.deg) / units.sday).to(units.arcminute/units.s)\n",
    "corr_FoV_min = Angle(10., units.degree)\n",
    "corr_FoV_max = Angle(90., units.degree)\n",
    "hera_latitude = Angle('-30:43:17.5', units.deg)\n",
    "corr_int_time = 0.1 * units.s\n",
    "corr_post_fs_int_time = 10. * units.s\n",
    "n_channels = 8192\n",
    "corr_chan_width = (250 * units.MHz) / n_channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bl_resolution(ew_bl_len):\n",
    "    # baseline E-W angular resolution\n",
    "    return Angle(np.arcsin(min(1, wavelength/(ew_bl_len * units.m))), units.radian).to(units.arcminute)\n",
    "\n",
    "def decorr_int_time(ew_bl_len):\n",
    "    # decorrelation due to pre-fringe-stopped integration time\n",
    "    bl_res = bl_resolution(ew_bl_len)\n",
    "    dit = corr_int_time * earth_rot_speed / bl_res.to(units.arcminute)\n",
    "    return dit.value\n",
    "\n",
    "def decorr_chan_width(ew_bl_len):\n",
    "    # decorrelation due to channel width\n",
    "    dcw = (corr_chan_width.to(1/units.s) * ew_bl_len * units.m\n",
    "           * np.sin(corr_FoV_min.to(units.rad)) / const.c)\n",
    "    return dcw.value\n",
    "\n",
    "def decorr_fringe_stop(ew_bl_len, fs_int_time):\n",
    "    # decorrelation due to averaging for a given post-fringe-stopped time\n",
    "    bl_res = bl_resolution(ew_bl_len)\n",
    "    dfs = (fs_int_time * np.sin(corr_FoV_min.radian) * earth_rot_speed\n",
    "           * np.abs(np.sin(hera_latitude)) / bl_res.to(units.arcminute))\n",
    "    return dfs.value\n",
    "\n",
    "def fs_int_time(ew_bl_len, decorr_val):\n",
    "    # fringe-stopped integration time for a given E-W separation and decorrelation value\n",
    "    bl_res = bl_resolution(ew_bl_len)\n",
    "    int_time = (decorr_val * bl_res.to(units.arcminute) / np.sin(corr_FoV_min.radian)\n",
    "                / earth_rot_speed / np.abs(np.sin(hera_latitude)))\n",
    "    return int_time.to(units.s).value\n",
    "\n",
    "def decorr_pre_fs(ew_bl_len):\n",
    "    # decorrelation from pre-fringe-stopped considerations (integration time + channel width)\n",
    "    return 1 - (1 - decorr_int_time(ew_bl_len)) * (1 - decorr_chan_width(ew_bl_len))\n",
    "\n",
    "def decorr_total(ew_bl_len, fs_int_time):\n",
    "    # pre-fringe-stopped + post-fringe-stopped decorrelation, given a post-fringe-stopped integration time\n",
    "    return 1 - (1 - decorr_pre_fs(ew_bl_len)) * (1 - decorr_fringe_stop(ew_bl_len, fs_int_time))\n",
    "\n",
    "def max_int_time(ew_bl_len, max_decorr):\n",
    "    # Compute the maximum post-fring-stopped integration time for a given E-W baseline\n",
    "    # length, in m, and max decorrelation fraction\n",
    "    # Assumes fringe stopping\n",
    "    dpf = decorr_pre_fs(ew_bl_len)\n",
    "    dfs = 1 - (1 - max_decorr)/(1 - dpf)\n",
    "    int_time = fs_int_time(ew_bl_len, dfs)\n",
    "\n",
    "    return int_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the file containing the proposed HERA-350 layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hera_txt = '/Users/plaplant/Documents/school/penn/software/hera_mc/hera_mc/data/HERA_350.txt'\n",
    "hera_bls = np.genfromtxt(hera_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compute the total savings for the array, which is driven by primarily by the E-W component of each baseline. We assume that if a given baseline has less than the maximum decorrelation level, we can continue to average until we reach it. We compute a \"perfect compression\" factor, which would be achievable by compressing a given baseline by the maximum amount. We also compute a \"simple\" compression factor, where the averaging is only done in a power-of-two fashion. (That is, we either perform no averaging, or average two consecutive time samples, or four, or eight, etc., while we are still below the maximum amount of decorrelation.) This approach allows for a more straightforward implementation, though obviously does not achieve the maximum savings available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum baseline separation found\n",
      "Maximum baseline separation found\n",
      "Maximum baseline separation found\n",
      "Maximum baseline separation found\n",
      "Maximum baseline separation found\n"
     ]
    }
   ],
   "source": [
    "# compute the savings for each baseline in the array\n",
    "# XXX: takes ~5 minutes; might be able to be more clever, but easier to shut up and calculate\n",
    "max_bda_data_rate = 0.\n",
    "simple_bda_data_rate = 0.\n",
    "\n",
    "nants = hera_bls.shape[0]\n",
    "for iant in range(nants):\n",
    "    # first column is antenna name; second--fourth columns are xyz positions, in meters\n",
    "    xi = hera_bls[iant, 1]\n",
    "    yi = hera_bls[iant, 2]\n",
    "    for jant in range(iant + 1, nants):\n",
    "        xj = hera_bls[jant, 1]\n",
    "        yj = hera_bls[jant, 2]\n",
    "\n",
    "        # To first order, E-W separation is given by delta-x value\n",
    "        # Prevent against division by zero\n",
    "        ew_sep = max(np.abs(xj - xi), 1e-1)\n",
    "\n",
    "        # compute total decorrelation for this baseline length and default correlator setting\n",
    "        dt = decorr_total(ew_sep, corr_post_fs_int_time)\n",
    "\n",
    "        if dt < max_decorr:\n",
    "            # we can theoretically integrate this bl in time until we hit the max_decorr\n",
    "            new_int_time = max_int_time(ew_sep, max_decorr)\n",
    "            max_bda_data_rate += corr_post_fs_int_time.value / new_int_time\n",
    "\n",
    "            # also compute the max power-of-two integration factor\n",
    "            fac = np.floor(np.log2(new_int_time / corr_post_fs_int_time.value))\n",
    "            simple_bda_data_rate += 2.**(-fac)\n",
    "            \n",
    "            # drive home the point that the default correlator output rates are overkill for most baselines\n",
    "            if fac == 0:\n",
    "                print(\"Maximum baseline separation found\")\n",
    "        else:\n",
    "            # no savings\n",
    "            max_bda_data_rate += 1\n",
    "            simple_bda_data_rate += 1\n",
    "\n",
    "# add factor for autos; assume no compression\n",
    "# note that it doesn't really matter, since these are < 1% of baselines\n",
    "max_bda_data_rate += nants\n",
    "simple_bda_data_rate += nants\n",
    "\n",
    "# normalize by the number of baselines\n",
    "nbls = (nants * (nants + 1)) / 2\n",
    "max_bda_data_rate /= nbls\n",
    "simple_bda_data_rate /= nbls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theoretic maximum baseline-dependent averaging savings for HERA-350 array: \n",
      "0.0659164291229 \n",
      "\n",
      "Power-of-2 savings: \n",
      "0.0929365079365\n"
     ]
    }
   ],
   "source": [
    "print(\"Theoretic maximum baseline-dependent averaging savings for HERA-350 array: \")\n",
    "print(max_bda_data_rate, \"\\n\")\n",
    "print(\"Power-of-2 savings: \")\n",
    "print(simple_bda_data_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the data rate above, we can compare the \"default\" data rate of the proposed correlator, and the \"actual\" one that includes the effect of BDA. For this calculation, we take the value corresponding to the \"simple\" compression scheme, which represents a more realistic value that can be achieved. The calculation above can be re-run for a different number of channels, but the compression savings are virtually the same. Thus, to convert the difference in data rate between, e.g., 8192 channels and 4096 channels, divide the rates and volumes below by a factor of 2 (or vice-versa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive data rate:       2.41532928 Gbyte / s\n",
      "Naive season volume:   12.5210669875 Pbyte \n",
      "\n",
      "BDA data rate:         0.2244722688 Gbyte / s\n",
      "BDA season volume:     1.16366424146 Pbyte\n"
     ]
    }
   ],
   "source": [
    "# compute data rate for season\n",
    "channels_to_keep = n_channels * 3. / 4.\n",
    "sum_diff_factor = 2\n",
    "bytes_per_vis = 8 * units.byte\n",
    "n_polarizations = 4\n",
    "obs_hrs_per_day = 12 * units.hour / units.day\n",
    "days_per_season = 120 * units.day\n",
    "\n",
    "naive_data_rate = (channels_to_keep * nbls * n_polarizations * bytes_per_vis\n",
    "                   * sum_diff_factor / corr_post_fs_int_time)\n",
    "naive_data_vol = naive_data_rate * obs_hrs_per_day * days_per_season\n",
    "bda_data_rate = simle_bda_data_rate * naive_data_rate\n",
    "bda_data_vol = bda_data_rate * obs_hrs_per_day * days_per_season\n",
    "print(\"Naive data rate:      \", naive_data_rate.to(units.Gbyte/units.s))\n",
    "print(\"Naive season volume:  \", naive_data_vol.to(units.Pbyte), \"\\n\")\n",
    "print(\"BDA data rate:        \", bda_data_rate.to(units.Gbyte/units.s))\n",
    "print(\"BDA season volume:    \", bda_data_vol.to(units.Pbyte))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
